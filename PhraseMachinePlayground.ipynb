{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PhraseMachinePlayground.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVav5SQQgBav",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "2bbefd75-d677-43bf-d591-d88ad4304fe9"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7ldh5DyfS1z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9c513a02-89d8-4e82-a48a-a06bd4ff7de0"
      },
      "source": [
        "# importing required libraries for functioning sentiment analysis system\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as plt\n",
        "import nltk\n",
        "from csv import reader\n",
        "from csv import DictReader\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download(\"stopwords\")\n",
        "from sklearn import neighbors\n",
        "from sklearn import svm\n",
        "from sklearn import datasets\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# defining equation to obtain an accuracy percentage for CNN\n",
        "def acc(confusion_matrix):\n",
        "  diagonal = confusion_matrix.trace()\n",
        "  elements = confusion_matrix.sum()\n",
        "  return diagonal/elements\n",
        "\n",
        "# determing the confidence of models through arrays\n",
        "cnn_x = []\n",
        "cnn_y = []\n",
        "svm_x = []\n",
        "svm_y = []\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZwp7dekga7H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "60403a60-7ed5-40cb-fb9d-8f8f9cba321c"
      },
      "source": [
        "# importing dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OI1h4Vwvi9OK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# defining and visualizing the dataset\n",
        "df = pd.read_csv('/content/drive/Shared drives/Hackathon/Division Sigma/training_data.csv')\n",
        "td = pd.read_csv('/content/drive/Shared drives/Hackathon/Division Sigma/contestant_judgment.csv')\n",
        "df = df[:30000]\n",
        "td = td[:30000]\n",
        "print(td)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUcoXYXr5mUq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Actually categorizing tweets\n",
        "\n",
        "KNN(df.Text, df.Sentiment)\n",
        "#SVM(svm_x, svm_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAz0fho7HO1C",
        "colab_type": "text"
      },
      "source": [
        "# SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZ8JcG5Yjo8o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def SVM(X_input,y_input):\n",
        "  # dividing data into a testing and training set\n",
        "  x_train = df.Text[:15000]\n",
        "  y_train = df.Text[:15000]\n",
        "  x_test = X_input\n",
        "  y_test = y_input\n",
        "\n",
        "\n",
        "  # converting data set into a form the program can read\n",
        "  cv = CountVectorizer(binary=False)\n",
        "  train_reviews = cv.fit_transform(x_train)\n",
        "  judgement_data = cv.transform(td.Text)\n",
        "\n",
        "  # converting sentiment from dataset to binary\n",
        "  lb=LabelBinarizer()\n",
        "  sentiment_data = lb.fit_transform(y_train)\n",
        "\n",
        "  # giving the model parameters\n",
        "  svm_model = svm.SVC(gamma=0.001, C=100)\n",
        "  svm_model.fit(train_reviews,sentiment_data)\n",
        "\n",
        "  # make predictions based on data set\n",
        "  prediction = svm_model.predict(test_reviews)\n",
        "  pojds = svm_model.predict(judgement_data)\n",
        "\n",
        "  print(\"Prediction:\")\n",
        "  print(svm_model.predict(train_reviews[-1]))\n",
        "  print('')\n",
        "  print(pojds)\n",
        "  print(\"Accuracy:\")\n",
        "  print(accuracy_score(x_test, prediction))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ph6mQ1RnubB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SVM(df.Text,df.Sentiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2cRmdwWHTAJ",
        "colab_type": "text"
      },
      "source": [
        "# KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7zd_Rm9HW8e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def KNN(X_input,y_input):\n",
        "  # dividing data into a testing and training set\n",
        "  x_train, x_test, y_train, y_test = train_test_split(X_input,y_input,test_size=0.33,random_state=42)\n",
        "\n",
        "  cv=CountVectorizer(binary=False)\n",
        "  train_reviews = cv.fit_transform(x_train)\n",
        "  test_reviews = cv.transform(x_test)\n",
        "  # converting sentiment from dataset to binary\n",
        "  lb=LabelBinarizer()\n",
        "  sentiment_data = lb.fit_transform(y_train)\n",
        "  sentiment_test = lb.fit_transform(y_test)\n",
        "  print(train_reviews.shape)\n",
        "\n",
        "  sentiment_chunk_1 = sentiment_data[:int(sentiment_data.shape[0] / 10 + 1)]\n",
        "  sentiment_chunk_2 = sentiment_data[int(sentiment_data.shape[0] / 10 + 1):int(sentiment_data.shape[0] / 5 + 1)]\n",
        "  sentiment_chunk_3 = sentiment_data[int(sentiment_data.shape[0] / 5 + 1):int(sentiment_data.shape[0] / 10 * 3 + 1)]\n",
        "  sentiment_chunk_4 = sentiment_data[int(sentiment_data.shape[0] / 10 * 3 + 1):int(sentiment_data.shape[0] / 10 * 4 + 1)]\n",
        "  sentiment_chunk_5 = sentiment_data[int(sentiment_data.shape[0] / 10 * 4 + 1):int(sentiment_data.shape[0] / 10 * 5 + 1)]\n",
        "  sentiment_chunk_6 = sentiment_data[int(sentiment_data.shape[0] / 10 * 5 + 1):int(sentiment_data.shape[0] / 10 * 6 + 1)]\n",
        "  sentiment_chunk_7 = sentiment_data[int(sentiment_data.shape[0] / 10 * 6 + 1):int(sentiment_data.shape[0] / 10 * 7 + 1)]\n",
        "  sentiment_chunk_8 = sentiment_data[int(sentiment_data.shape[0] / 10 * 7 + 1):int(sentiment_data.shape[0] / 10 * 8 + 1)]\n",
        "  sentiment_chunk_9 = sentiment_data[int(sentiment_data.shape[0] / 10 * 8 + 1):int(sentiment_data.shape[0] / 10 * 9 + 1)]\n",
        "  sentiment_chunk_10 = sentiment_data[int(sentiment_data.shape[0] / 10 * 9 + 1):]\n",
        "\n",
        "  chunk_1 = train_reviews[:int(train_reviews.shape[0] / 10 + 1)]\n",
        "  chunk_2 = train_reviews[int(train_reviews.shape[0] / 10 + 1):int(train_reviews.shape[0] / 5 + 1)]\n",
        "  chunk_3 = train_reviews[int(train_reviews.shape[0] / 5 + 1):int(train_reviews.shape[0] / 10 * 3 + 1)]\n",
        "  chunk_4 = train_reviews[int(train_reviews.shape[0] / 10 * 3 + 1):int(train_reviews.shape[0] / 10 * 4 + 1)]\n",
        "  chunk_5 = train_reviews[int(train_reviews.shape[0] / 10 * 4 + 1):int(train_reviews.shape[0] / 10 * 5 + 1)]\n",
        "  chunk_6 = train_reviews[int(train_reviews.shape[0] / 10 * 5 + 1):int(train_reviews.shape[0] / 10 * 6 + 1)]\n",
        "  chunk_7 = train_reviews[int(train_reviews.shape[0] / 10 * 6 + 1):int(train_reviews.shape[0] / 10 * 7 + 1)]\n",
        "  chunk_8 = train_reviews[int(train_reviews.shape[0] / 10 * 7 + 1):int(train_reviews.shape[0] / 10 * 8 + 1)]\n",
        "  chunk_9 = train_reviews[int(train_reviews.shape[0] / 10 * 8 + 1):int(train_reviews.shape[0] / 10 * 9 + 1)]\n",
        "  chunk_10 = train_reviews[int(train_reviews.shape[0] / 10 * 9 + 1):]\n",
        "\n",
        "  chunks = [chunk_1, chunk_2, chunk_3, chunk_4, chunk_5, chunk_6, chunk_7, chunk_8, chunk_9, chunk_10]\n",
        "\n",
        "  # this uses the neighbor method to get a working algorithim \n",
        "  knn = neighbors.KNeighborsClassifier(n_neighbors=115, weights = 'uniform')\n",
        "\n",
        "  knn.fit(train_reviews, sentiment_data)\n",
        "\n",
        "  prediction1 = knn.predict(chunk_1)\n",
        "  prediction2 = knn.predict(chunk_2)\n",
        "  prediction3 = knn.predict(chunk_3)\n",
        "  prediction4 = knn.predict(chunk_4)\n",
        "  prediction5 = knn.predict(chunk_5)\n",
        "  prediction6 = knn.predict(chunk_6)\n",
        "  prediction7 = knn.predict(chunk_7)\n",
        "  prediction8 = knn.predict(chunk_8)\n",
        "  prediction9 = knn.predict(chunk_9)\n",
        "  prediction10 = knn.predict(chunk_10)\n",
        "\n",
        "  preds = [prediction1, prediction2, prediction3, prediction4, prediction5, prediction6, prediction7, prediction8, prediction9, prediction10]\n",
        "  pred_accs = []\n",
        "\n",
        "  sents = (sentiment_chunk_1, sentiment_chunk_2, sentiment_chunk_3, sentiment_chunk_4, sentiment_chunk_5, sentiment_chunk_6, sentiment_chunk_7, sentiment_chunk_8, sentiment_chunk_9, sentiment_chunk_10)\n",
        "\n",
        "  i = -1\n",
        "  for pred in preds:\n",
        "    i += 1\n",
        "    pred_accs.append(int(accuracy_score(sents[i], preds[i]) * 100))\n",
        "\n",
        "  lowest_acc = 0\n",
        "  lowest_ind = 0\n",
        "  second_lowest = 0\n",
        "  second_ind = 0\n",
        "\n",
        "  i = 0\n",
        "\n",
        "  for acc in pred_accs:\n",
        "    i += 1\n",
        "    if acc < lowest_acc or lowest_acc == 0:\n",
        "      second_lowest = lowest_acc\n",
        "      second_ind = lowest_ind\n",
        "      lowest_acc = pred_accs[i]\n",
        "      lowest_ind = i\n",
        "\n",
        "  lowest = df[int(df.shape[0] / 10 * lowest_ind + 1):int(df.shape[0] / 10 * lowest_ind + 1) + 1]\n",
        "  second = df[int(df.shape[0] / 10 * second_ind + 1):int(df.shape[0] / 10 * second_ind + 1) + 1]\n",
        "\n",
        "  svb,_x = lowest.Text + second.Text\n",
        "  cnn_y = lowest.Sentiment + second.Sentiment\n",
        "\n",
        "  SVM(pd.Series(svm_x), pd.Series(svm_y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7rxghGboNhP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "c2f95337-6699-4b60-c2b8-ab026588bdfa"
      },
      "source": [
        "KNN(df.Text,df.Sentiment)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20100, 30592)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:85: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-bf2096a2a8cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mKNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mText\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-fcb834346548>\u001b[0m in \u001b[0;36mKNN\u001b[0;34m(X_input, y_input)\u001b[0m\n\u001b[1;32m     83\u001b[0m   \u001b[0mcnn_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlowest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentiment\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msecond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m   \u001b[0mSVM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvm_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvm_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-22617f5458cf>\u001b[0m in \u001b[0;36mSVM\u001b[0;34m(X_input, y_input)\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;31m# giving the model parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0msvm_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   \u001b[0msvm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_reviews\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentiment_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0;31m# make predictions based on data set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    146\u001b[0m         X, y = check_X_y(X, y, dtype=np.float64,\n\u001b[1;32m    147\u001b[0m                          \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m                          accept_large_sparse=False)\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    758\u001b[0m                         dtype=None)\n\u001b[1;32m    759\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 760\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    761\u001b[0m         \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_numeric\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'O'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, warn)\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bad input shape {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: bad input shape (15000, 14986)"
          ]
        }
      ]
    }
  ]
}