{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PhraseMachinePlayground.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVav5SQQgBav",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "2bbefd75-d677-43bf-d591-d88ad4304fe9"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7ldh5DyfS1z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "409a7362-d4bf-463e-c5ac-85701ab8f965"
      },
      "source": [
        "# importing required libraries for functioning sentiment analysis system\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as plt\n",
        "import nltk\n",
        "from csv import reader\n",
        "from csv import DictReader\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download(\"stopwords\")\n",
        "from sklearn import neighbors\n",
        "from sklearn import svm\n",
        "from sklearn import datasets\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# defining equation to obtain an accuracy percentage for CNN\n",
        "def acc(confusion_matrix):\n",
        "  diagonal = confusion_matrix.trace()\n",
        "  elements = confusion_matrix.sum()\n",
        "  return diagonal/elements\n",
        "\n",
        "# determing the confidence of models through arrays\n",
        "cnn_x = []\n",
        "cnn_y = []\n",
        "svm_x = []\n",
        "svm_y = []\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZwp7dekga7H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2550d069-9cdc-42be-a4df-ce448278c790"
      },
      "source": [
        "# importing dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OI1h4Vwvi9OK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "c38b12b2-2a48-454c-b357-397b86a0bdcc"
      },
      "source": [
        "# defining and visualizing the dataset\n",
        "df = pd.read_csv('/content/drive/Shared drives/Hackathon/Division Sigma/training_data.csv')\n",
        "td = pd.read_csv('/content/drive/Shared drives/Hackathon/Division Sigma/contestant_judgment.csv')\n",
        "df = df[:30000]\n",
        "td = td[:30000]\n",
        "print(td)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "            ID  ...                                               Text\n",
            "0       599303  ...  I'm on my way to miss kacy's 4th bday party at...\n",
            "1       359673  ...  @ripestapple  I might not be the right person ...\n",
            "2       391095  ...                           @zomgkris I know it is. \n",
            "3       820049  ...  Mii ViSioN is BLuRRy...iM goiN to Bed!!NiTe Ni...\n",
            "4       658429  ...  @tealou anyways - i did something good for som...\n",
            "...        ...  ...                                                ...\n",
            "29995   983217  ...  @madnewsblog why do you not like her? Miquita ...\n",
            "29996   561169  ...  having issues getting ratpoison to work on mac...\n",
            "29997   639109  ...   I Wish This Trip Was Under Better Circumstances \n",
            "29998  1319452  ...  &quot;HH&quot; is coming along. working on the...\n",
            "29999  1197631  ...  http://bit.ly/17zq9D  Gayest thing i've seen i...\n",
            "\n",
            "[30000 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUcoXYXr5mUq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "696c7984-94ad-4592-c48f-9652e7bd1dda"
      },
      "source": [
        "# Actually categorizing tweets\n",
        "\n",
        "KNN(df.Text, df.Sentiment)\n",
        "#SVM(svm_x, svm_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20100, 30592)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAz0fho7HO1C",
        "colab_type": "text"
      },
      "source": [
        "# SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZ8JcG5Yjo8o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def SVM(X_input,y_input):\n",
        "  # dividing data into a testing and training set\n",
        "  x_train = df.Text[:15000]\n",
        "  y_train = df.Text[:15000]\n",
        "  x_test = X_input\n",
        "  y_test = y_input\n",
        "  \n",
        "  print('1')\n",
        "\n",
        "  # converting data set into a form the program can read\n",
        "  cv = CountVectorizer(binary=False)\n",
        "  train_reviews = cv.fit_transform(x_train)\n",
        "  judgement_data = cv.transform(td.Text)\n",
        "  print()\n",
        "\n",
        "  # converting sentiment from dataset to binary\n",
        "  lb=LabelBinarizer()\n",
        "  sentiment_data = lb.fit_transform(y_train)\n",
        "\n",
        "  # giving the model parameters\n",
        "  svm_model = svm.SVC(gamma=0.001, C=100)\n",
        "  svm_model.fit(train_reviews,sentiment_data)\n",
        "\n",
        "  # make predictions based on data set\n",
        "  prediction = svm_model.predict(x_test)\n",
        "  pojds = svm_model.predict(judgement_data)\n",
        "\n",
        "  print(\"Prediction:\")\n",
        "  print(svm_model.predict(train_reviews[-1]))\n",
        "  print('')\n",
        "  print(pojds)\n",
        "  print(\"Accuracy:\")\n",
        "  print(accuracy_score(x_test, prediction))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ph6mQ1RnubB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SVM(df.Text,df.Sentiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2cRmdwWHTAJ",
        "colab_type": "text"
      },
      "source": [
        "# KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7zd_Rm9HW8e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def KNN(X_input,y_input):\n",
        "  # dividing data into a testing and training set\n",
        "  x_train, x_test, y_train, y_test = train_test_split(X_input,y_input,test_size=0.33,random_state=42)\n",
        "\n",
        "  cv=CountVectorizer(binary=False)\n",
        "  train_reviews = cv.fit_transform(x_train)\n",
        "  test_reviews = cv.transform(x_test)\n",
        "  # converting sentiment from dataset to binary\n",
        "  lb=LabelBinarizer()\n",
        "  sentiment_data = lb.fit_transform(y_train)\n",
        "  sentiment_test = lb.fit_transform(y_test)\n",
        "  print(train_reviews.shape)\n",
        "\n",
        "  sentiment_chunk_1 = sentiment_data[:int(sentiment_data.shape[0] / 10 + 1)]\n",
        "  sentiment_chunk_2 = sentiment_data[int(sentiment_data.shape[0] / 10 + 1):int(sentiment_data.shape[0] / 5 + 1)]\n",
        "  sentiment_chunk_3 = sentiment_data[int(sentiment_data.shape[0] / 5 + 1):int(sentiment_data.shape[0] / 10 * 3 + 1)]\n",
        "  sentiment_chunk_4 = sentiment_data[int(sentiment_data.shape[0] / 10 * 3 + 1):int(sentiment_data.shape[0] / 10 * 4 + 1)]\n",
        "  sentiment_chunk_5 = sentiment_data[int(sentiment_data.shape[0] / 10 * 4 + 1):int(sentiment_data.shape[0] / 10 * 5 + 1)]\n",
        "  sentiment_chunk_6 = sentiment_data[int(sentiment_data.shape[0] / 10 * 5 + 1):int(sentiment_data.shape[0] / 10 * 6 + 1)]\n",
        "  sentiment_chunk_7 = sentiment_data[int(sentiment_data.shape[0] / 10 * 6 + 1):int(sentiment_data.shape[0] / 10 * 7 + 1)]\n",
        "  sentiment_chunk_8 = sentiment_data[int(sentiment_data.shape[0] / 10 * 7 + 1):int(sentiment_data.shape[0] / 10 * 8 + 1)]\n",
        "  sentiment_chunk_9 = sentiment_data[int(sentiment_data.shape[0] / 10 * 8 + 1):int(sentiment_data.shape[0] / 10 * 9 + 1)]\n",
        "  sentiment_chunk_10 = sentiment_data[int(sentiment_data.shape[0] / 10 * 9 + 1):]\n",
        "\n",
        "  chunk_1 = train_reviews[:int(train_reviews.shape[0] / 10 + 1)]\n",
        "  chunk_2 = train_reviews[int(train_reviews.shape[0] / 10 + 1):int(train_reviews.shape[0] / 5 + 1)]\n",
        "  chunk_3 = train_reviews[int(train_reviews.shape[0] / 5 + 1):int(train_reviews.shape[0] / 10 * 3 + 1)]\n",
        "  chunk_4 = train_reviews[int(train_reviews.shape[0] / 10 * 3 + 1):int(train_reviews.shape[0] / 10 * 4 + 1)]\n",
        "  chunk_5 = train_reviews[int(train_reviews.shape[0] / 10 * 4 + 1):int(train_reviews.shape[0] / 10 * 5 + 1)]\n",
        "  chunk_6 = train_reviews[int(train_reviews.shape[0] / 10 * 5 + 1):int(train_reviews.shape[0] / 10 * 6 + 1)]\n",
        "  chunk_7 = train_reviews[int(train_reviews.shape[0] / 10 * 6 + 1):int(train_reviews.shape[0] / 10 * 7 + 1)]\n",
        "  chunk_8 = train_reviews[int(train_reviews.shape[0] / 10 * 7 + 1):int(train_reviews.shape[0] / 10 * 8 + 1)]\n",
        "  chunk_9 = train_reviews[int(train_reviews.shape[0] / 10 * 8 + 1):int(train_reviews.shape[0] / 10 * 9 + 1)]\n",
        "  chunk_10 = train_reviews[int(train_reviews.shape[0] / 10 * 9 + 1):]\n",
        "\n",
        "  chunks = [chunk_1, chunk_2, chunk_3, chunk_4, chunk_5, chunk_6, chunk_7, chunk_8, chunk_9, chunk_10]\n",
        "\n",
        "  # this uses the neighbor method to get a working algorithim \n",
        "  knn = neighbors.KNeighborsClassifier(n_neighbors=115, weights = 'uniform')\n",
        "\n",
        "  knn.fit(train_reviews, sentiment_data)\n",
        "\n",
        "  prediction1 = knn.predict(chunk_1)\n",
        "  prediction2 = knn.predict(chunk_2)\n",
        "  prediction3 = knn.predict(chunk_3)\n",
        "  prediction4 = knn.predict(chunk_4)\n",
        "  prediction5 = knn.predict(chunk_5)\n",
        "  prediction6 = knn.predict(chunk_6)\n",
        "  prediction7 = knn.predict(chunk_7)\n",
        "  prediction8 = knn.predict(chunk_8)\n",
        "  prediction9 = knn.predict(chunk_9)\n",
        "  prediction10 = knn.predict(chunk_10)\n",
        "\n",
        "  preds = [prediction1, prediction2, prediction3, prediction4, prediction5, prediction6, prediction7, prediction8, prediction9, prediction10]\n",
        "  pred_accs = []\n",
        "\n",
        "  sents = (sentiment_chunk_1, sentiment_chunk_2, sentiment_chunk_3, sentiment_chunk_4, sentiment_chunk_5, sentiment_chunk_6, sentiment_chunk_7, sentiment_chunk_8, sentiment_chunk_9, sentiment_chunk_10)\n",
        "\n",
        "  i = -1\n",
        "  for pred in preds:\n",
        "    i += 1\n",
        "    pred_accs.append(int(accuracy_score(sents[i], preds[i]) * 100))\n",
        "\n",
        "  lowest_acc = 0\n",
        "  lowest_ind = 0\n",
        "  second_lowest = 0\n",
        "  second_ind = 0\n",
        "\n",
        "  i = 0\n",
        "\n",
        "  for acc in pred_accs:\n",
        "    i += 1\n",
        "    if acc < lowest_acc or lowest_acc == 0:\n",
        "      second_lowest = lowest_acc\n",
        "      second_ind = lowest_ind\n",
        "      lowest_acc = pred_accs[i]\n",
        "      lowest_ind = i\n",
        "\n",
        "  lowest = df[int(df.shape[0] / 10 * lowest_ind + 1):int(df.shape[0] / 10 * lowest_ind + 1) + 1]\n",
        "  second = df[int(df.shape[0] / 10 * second_ind + 1):int(df.shape[0] / 10 * second_ind + 1) + 1]\n",
        "\n",
        "  svb,_x = lowest.Text + second.Text\n",
        "  cnn_y = lowest.Sentiment + second.Sentiment\n",
        "\n",
        "  SVM(pd.Series(svm_x), pd.Series(svm_y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7rxghGboNhP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "ae7b7a70-08a7-419c-aea3-b7b342d131e7"
      },
      "source": [
        "KNN(df.Text,df.Sentiment)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-bf2096a2a8cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mKNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mText\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    }
  ]
}